---
title: "5 R lab"
author: "Yijun Shen"
date: '2022-06-06'
output: html_document
---
#They are basically resampling trchnic



##The Validation Set Approach
Simply randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. 
```{r }
library(tidyverse)
library(ISLR)
set.seed (1)
train=sample(392,196)#set train dataset, 196 out of 392 original observations
view(Auto)#use the Auto dataset in tidyverse


lm.fit=lm(mpg~horsepower, data=Auto, subset=train)# create a lm to use only observations from the training set

attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
#use predict() function to estimate the response for all 392 observations, then use mean() function to calcilate MSE in the validation set(test set) for 196 observations
#-train means observations that are not in the train set. 


#now we use poly() function to estimate the test error for cubic and polynomial regressions:
lm.fit2=lm(mpg~poly(horsepower ,2),data=Auto,subset=train)

mean((mpg-predict(lm.fit2,Auto))[-train]^2)

lm.fit3=lm(mpg~poly(horsepower ,3),data=Auto,subset=train)

mean((mpg-predict(lm.fit3,Auto))[-train]^2)

```



##k-Fold Cross-Validation
randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. Repeated K times and calculate MSE for each time, and then average the MSE values. Leave-One-Out Cross-Validation is simply make k equal to n, which take only one observation as test set. 
```{r cars}
library(boot)
set.seed(17)
cv.error.10=rep(0,10)#k=10

#use glm function to implement K-fold cv
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}

cv.error.10

mean(cv.error.10)
```




##The Bootstrap
obtain distinct data sets by repeatedly sampling observations from the original data set.randomly select n observations from the data set in order to produce a bootstrap data set, Z∗1. The sampling is performed with replacement, which means that the same observation can occur more than once in the bootstrap data set.

```{r}
#create a function that computes the statistic of interest:
 alpha.fn=function(data,index){
   X=data$X[index]
   Y=data$Y[index]
   return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
 }
#This function returns an estimate for α based on applying (5.7) to the observations indexed by the argument index.

alpha.fn(Portfolio ,1:100)#estimate α using 100 observations


set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))#randomly take 100 samples forom 1:100 with replacement
#This is  like to constructing a new bootstrap data set and recomputing αˆ based on the new data set. we try to obtain all estimate αˆ and use to calculate the resulting SD

boot(Portfolio ,alpha.fn,R=1000)#boot function from boot library

```

##Estimating the Accuracy of a Linear Regression Model using Boostrap
We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of 392 observations in order to compute the esti- mates of β0 and β1 on the entire data set using the usual linear regression coefficient estimate formulas
```{r}
boot.fn=function(data, index)
  return(coef(lm(mpg~horsepower, data=data, subset=index)))#return linear regression coef

boot.fn(Auto, 1:392)


#also can use with replacement, ex:

set.seed(1)
boot.fn(Auto, sample(392,392,replace=T))


#can also use boot() function to compute SE of 1000 bootstrap estimates for intercept and slope

boot(Auto, boot.fn, 1000)

#standard formulas can be used to compute the standard errors for the regression coefficients in a linear model:
summary(lm(mpg~horsepower ,data=Auto))$coef
```
